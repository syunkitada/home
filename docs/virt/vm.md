# 基礎


## QEMU
* Quick Emulator
* CPU、ディスク、NICなどのハードウェアをすべてエミュレートする仮想マシンエミュレータ
    * QEMUはすべてのハードウェアをエミュレート(完全仮想化)できるのが、これは処理が多くなり効率的ではない
    * そのため、CPUはkvmや仮想支援機構を利用してCPU上で仮想マシンのコードを直接実行したり、ネットワーク処理はカーネルの機能に任せるなど、すべてを仮想化せずに一部エミュレートする(準仮想化)


## CPUのエミュレーション
* リングプロテクション
    * リングプロテクションとは、CPUの特権レベルのことで0～3まである。このレベルによって利用できるCPUの命令が異なる
    * ユーザプロセスはリング3で動き利用できる命令に制限があり、kernelはリング0で動き命令の制限がない
* センシティブ命令
    * CPUの命令には、計算だけでなく、IOなどハードウェアを使う、センシティブ命令がある
    * 仮想マシンでセンシティブ命令が実行された場合、CPUはこれをフックしてQEMUなどのエミュレータがこれを適切にエミュレートする必要がある
* 仮想化支援機構とKVM
    * 従来の仮想マシンは、リング3で動いていた
        * 仮想マシンはあくまでユーザプロセスなので、リング3で動かす必要があった
        * そこで、OSを修正したり、VMM(仮想マシンモニタ)が命令を修正したりしてリング3で動くようにしていた
    * 仮想化支援機構(Intel VT-x AMD AMD-V)
        * CPUで仮想マシン用の実行モードをサポートすることで、仮想マシンのコードがリングプロテクションのもとで実行され、センシティブ命令時にはこれをトラップしてエミュレートできるようにした
        * CPUの実行モードには、通常のリングプロテクションの実行モード(VMX rootモード)のほかに、仮想マシン実行用のモード(VMX nonrootモード)が追加された
        * 仮想マシンのコードが実行されるときは、CPUをVMX nonrootモードに切り替え(VM Enter)、CPUで仮想マシンのコードを直接実行する
            * 仮想マシンのkernelはVMX nonrootモードのリング0で実行され、ユーザプロセスはVMX nonrootモードのリング3で実行される
        * センシティブな命令が実行された場合には、いったんVMX rootモードになって(VM Exit)、kvmがその制御を行う
            * kvmは、ネットワークなどのI/O処理はカーネル空間上で処理が完結させるが、ディスクI/Oなどはkvmでは扱えず、QEMUなどのエミュレータに処理を依頼する
            * kvmが処理を完了すると、VMX nonrootモードに復帰して(VM Enter)、仮想マシンのコードが再び実行される
    * KVM
        * Kernel-based Virtual Machine
        * 仮想化支援機構を使って仮想マシン機能を提供するカーネルモジュール
* VMの実行時間(guest時間)
    * ホスト側からmpstatを実行すると確認できる
        * QEMUがセンシティブ命令などをエミュレートしてる時間がuser時間
        * kernel(kvmなど)の実行時間がsys時間
        * VMX non-rootモードでVMが実行してる時間がguest時間
* VMのsteal time
    * VM側からmpstatを実行すると確認できる
    * VMがプロセスをスケジュールするとき、vcpuにCPU時間を割り当てるが、実際にこれが実行されるのは、ホストがvcpuに実CPUの時間を割り当てられる時である
    * このとき、実際に実行されたcpuの時間と、VMが実行しようとしたvcpuの時間の差分が、steal timeとなる
    * steal timeは、vcpuのmsrレジスタ経由で情報をkvm.koから受け取り計算される
        * msrレジスタは、雑多な処理をするためのレジスタ


## KVM
* /dev/kvm
    * KVMのインターフェイスになる特殊デバイス
    * ioctl()で様々な機能を提供
* 仮想マシン作成の流れ
    * /dev/kvmで仮想マシンを作成すると、kvm-vmファイル記述子が返される
    * kvm-vmを使って、メモリを割り当てる
    * kvm-vmを使って、vcpuをつくると、kvm-vcpuというファイル記述子が返され、1つのvcpuごとにvcpuスレッド作成して管理する
    * kvm-vcpuを使って、vcpuのレジスタ(pcなど)に値をセットする
    * kvm-vcpuを使って、センシティブ命令(I/O)発生時のハンドラをセットする
    * kvm-vcpuを使って、vcpuをスタートする
    * keyboard入力などの割り込みを発生させる場合は、kvm-vcpuを使って、vcpuに割り込みを入れる


## 仮想マシンのメモリ管理
* VMでもOSがページテーブルを管理しており、仮想アドレスを物理アドレスに変換している
* しかし、VMの物理アドレスというのは、QEMUで管理している仮想メモリ空間である実際のメモリアドレスとは異なるため、VMの物理アドレスをホストの物理アドレスに変換する仕組みが必要である
* シャドウページング
    * ホストはシャドウページテーブルという仮想アドレスと物理アドレスを変換できるページテーブルを作る
    * これとVMのページテーブルを同期させることで、VMからのメモリアクセスを可能にする
    * しかし、メモリの読み書きにいちいち同期をとる必要があるパフォーマンスがよくない
* EPT(Extended Page Table: Intel)、NPT(Nested Page Table: AMD)
    * EPTはMMUの拡張機能で、VM物理アドレスをホスト物理アドレスに変換する機能をプロセッサレベルで提供する
        * EPTはVM物理アドレス(GPA)からホスト物理アドレス(HPA)への変換を行う4段のページテーブル
        * 仕組みは通常のページテーブル機構とほぼ同じで、TLBも使うし、ページサイズが大きくなればページウォーク数も減る
    * まだテーブルにマッピングされていないアドレスへのアクセスが発生した場合EPT Violationが発生しVMExitしてページフォルトが発生する
    * VM物理アドレスと物理アドレスをマッピングした情報はVMCSのEPTP(EPT Pointer)に登録しておき、MMUがこれを使って物理メモリにアクセスする
    * VMが仮想アドレスにアクセスすると、MMUがページテーブルによりVM物理アドレスに変換し、MMUがEPTにより物理アドレスに変換して物理アドレスに変換する
* THPの利用による高速化
    * ホストでTHPを有効にしておくと、QEMUがページを確保する際に2Mのページサイズとなる
    * EPTでのページウォーク数も4から3になり、EPTでのTLBヒット率も上がり、ページテーブルのサイズも節約される
    * THPのデメリット
        * ページサイズを2Mにすると、ページフォルト時のページ初期化に時間がかかる
        * 仮想メモリのサイズが肥大化しやすい(実サイズは肥大化しないから問題ない?)
* Hugetlbfsによる高速化
    * THPが2M固定なのに対して、2Mもしくは1Gのページサイズを割り当てることができる
    * ページサイズを1Gにすれば、EPTでのページウォーク数も4から2になり、EPTでのTLBヒット率も上がり、ページテーブルサイズも節約される
    * THPとの使い分け
        * Hugetlbfsは通常のメモリ空間とは隔離され、スワップアウトできず、ブート時に静的に確保しないといけない
        * 稼働させようとしているVM数とそのメモリ利用量が見積れて、余裕があるならHugetlbfsを使うのが良い
        * そうでないなら、THPのほうがメモリの融通が利くので、THPのほうが良い
* 参考
    * [Red Hat: Transparent Hugepage Support](https://www.linux-kvm.org/images/9/9e/2010-forum-thp.pdf)
        * THPの説明、THPの有効/無効時、EPT有効/無効時のベンチマークが乗ってる


## 参考
* https://lwn.net/Articles/658511/
* [Linux KVMのコードを追いかけてみよう] (http://www.slideshare.net/ozax86/linux-kvm?qid=fb99f565-8ae4-44d3-9b58-8d8487197566&v=&b=&from_search=26)
