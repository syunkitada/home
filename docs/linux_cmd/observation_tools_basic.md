# Observability tools basic

PCのリソースの利用状況や正常性、エラーをチェックするためのコマンド集です。

## sysstatのインストール
``` bash
$ sudo yum install sysstat
```


## uptime
``` bash
# 現在の時刻 up 起動時間, ログインユーザ数 user, load avelage: 1分, 5分, 30分
$ uptime
23:51:26 up 21:31,  1 user,  load average: 30.02, 26.43, 19.02
```

* 現在の時刻がずれている場合
dateでタイムゾーンをチェック
``` bash
$ date
2016年  2月 28日 日曜日 11:55:54 JST
```

ntpで時刻同期をチェック
``` bash
$ ntpq -p
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 ntp.kiba.net    189.130.221.188  2 u   31   64    1   24.292   -0.229   0.000
 ec2-54-64-6-78. 133.243.238.164  2 u   31   64    1   17.889    0.627   0.000
```

ntpの設定をチェック
``` bash
$ vim -R /etc/ntp.conf
```

* 起動時間が短い場合
リブートした覚えがないのに、これが短い場合PCが何かの原因でリブートした可能性がある

* load avelage
load avelageとは、現在CPUが実行しているタスク数のこと。
つまり、CPUのスレッド数よりも大きい場合は、処理が間に合っていないことになる。

これには、割り込み不可能なIOでブロックされているプロセスも含まれているので、ディスクIOが間に合っていない場合などによくこの値が高くなる。

また、load avelageが、1分間、5分間、15分間の指数移動平均で表示されるので、負荷が時間経過とともにどのように変化したかがわかる。

15分間のload averageより、5, 1分間のload averageが非常に大きい場合、今現在なにかが起きている可能性がある。
また、5, 1分間のload averageが非常に少ない場合、今現在はなにも起きていないが、5～15分前になにか起きていた可能性がある。


## dmesg | less

``` bash
$ dmesg | less

[1880957.563150] perl invoked oom-killer: gfp_mask=0x280da, order=0, oom_score_adj=0
[...]
[1880957.563400] Out of memory: Kill process 18694 (perl) score 246 or sacrifice child
[1880957.563408] Killed process 18694 (perl) total-vm:1972392kB, anon-rss:1953348kB, file-rss:0kB
[2320864.954447] TCP: Possible SYN flooding on port 7001. Dropping request.  Check SNMP counters.
```
oom-killerとTCPのリクエストのドロップなど、エラーを含むシステムの情報を得ることができます。
なにか起きてると思ったらすぐにチェックすべき項目です。


## vmstat 1 -t
``` bash
$ vmstat 1 -t
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- -----timestamp-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st                 UTC
 2  0      0 259516    888 1476192    0    0    84  1514 1191 2660 10  4 83  1  2 2017-03-04 03:35:53
 5  0      0 259032    888 1476224    0    0     0    19 1015 2444  7  3 90  0  0 2017-03-04 03:35:54
 0  0      0 259268    888 1476228    0    0     0    25 1253 2774 16  8 69  0  6 2017-03-04 03:35:55
 0  0      0 259236    888 1476236    0    0     0    46  993 2469  6  1 93  0  0 2017-03-04 03:35:56
 0  0      0 259004    888 1476252    0    0     0    27 1168 2941  5  5 88  0  2 2017-03-04 03:35:57
^C
```

1秒間ごとに仮想メモリの統計を表示します。
一行目はシステム起動時からの平均。

r : CPUで実行中および順番を待っているプロセスの数。これはI/Oを含んでいないので、CPUの飽和状態を見るのに、ロードアベレージよりも良いシグナルになります。言い換えると、"r"の値がCPU数よりも多ければ飽和状態ということです。

free : キロバイトでの空きメモリー量。数えられないぐらいの桁数が表示されていたら、十分なメモリーがあります。7番目に出てくるfree -mコマンドは、空きメモリのより詳しい説明を表示してくれます。

si, so : スワップインとスワップアウト。ゼロでない値があれば、メモリ不足。

us, sy, id, wa, st : CPU時間の内訳で、すべてのCPUに対する平均値。それぞれ、ユーザ時間、システム(カーネル)時間、アイドル時間、I/O待ち時間、stealされた時間(他のゲストマシンや、Xenの場合ゲストの分離されたドライバードメインによるsteal)。


CPU時間の内訳で、userとsystem時間を足すことでCPUがビジーかどうか確認できるでしょう。I/O待ちが一定の数値を示しているならディスクがボトルネックです。この時、タスクはディスクI/O待ちでブロックされてしまうため、CPUはアイドル状態になってしまっています。従って、I/O待ちはCPUアイドル時間の別の形と考えられ、なぜアイドルなのかを調べる手がかりになり得ます。

システム時間は、I/O処理に必要です。20%を超えるような高いシステム時間は、詳しく調べる必要があると言えるでしょう。おそらくカーネルがI/Oを効率よく処理できていない状態です。

上の例では、CPU時間はほとんどユーザレベルになっており、つまりアプリケーションレベルが使用していることを示しています。CPUは平均90%以上使用されています。これは必ずしも問題とは言えず、「r」列で飽和状態の程度を調べましょう。


## mpstat -P ALL 1
``` bash
$ mpstat -P ALL 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)

07:38:49 PM  CPU   %usr  %nice   %sys %iowait   %irq  %soft  %steal  %guest  %gnice  %idle
07:38:50 PM  all  98.47   0.00   0.75    0.00   0.00   0.00    0.00    0.00    0.00   0.78
07:38:50 PM    0  96.04   0.00   2.97    0.00   0.00   0.00    0.00    0.00    0.00   0.99
07:38:50 PM    1  97.00   0.00   1.00    0.00   0.00   0.00    0.00    0.00    0.00   2.00
07:38:50 PM    2  98.00   0.00   1.00    0.00   0.00   0.00    0.00    0.00    0.00   1.00
07:38:50 PM    3  96.97   0.00   0.00    0.00   0.00   0.00    0.00    0.00    0.00   3.03
[...]
```

CPUごとのCPU時間の内訳を表示します。

top中に 1 でも見れる。


## pidstat 1
``` bash
$ pidstat 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015    _x86_64_    (32 CPU)

07:41:02 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
07:41:03 PM     0         9    0.00    0.94    0.00    0.94     1  rcuos/0
07:41:03 PM     0      4214    5.66    5.66    0.00   11.32    15  mesos-slave
07:41:03 PM     0      4354    0.94    0.94    0.00    1.89     8  java
07:41:03 PM     0      6521 1596.23    1.89    0.00 1598.11    27  java
07:41:03 PM     0      6564 1571.70    7.55    0.00 1579.25    28  java
07:41:03 PM 60004     60154    0.94    4.72    0.00    5.66     9  pidstat

07:41:03 PM   UID       PID    %usr %system  %guest    %CPU   CPU  Command
07:41:04 PM     0      4214    6.00    2.00    0.00    8.00    15  mesos-slave
07:41:04 PM     0      6521 1590.00    1.00    0.00 1591.00    27  java
07:41:04 PM     0      6564 1573.00   10.00    0.00 1583.00    28  java
07:41:04 PM   108      6718    1.00    0.00    0.00    1.00     0  snmp-pass
07:41:04 PM 60004     60154    1.00    4.00    0.00    5.00     9  pidstat
^C
```
pidstatは、topのプロセスごとの概要とも言えるものですが、スクリーンをクリアする代わりに連続して概要を表示します。これは、時系列でのパターンを見るのに便利で、見たものを調査の記録にとっておく(コピペ)のにもよいでしょう。

上の例では、2つのjavaプロセスがCPUを消費している原因だとわかります。%CPU列は全CPUに対する使用率ですが、1591%という表示からjavaプロセスがほぼ16CPU分を使用していると分かります。



## iostat -xz 1
``` bash
$ iostat -xz 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015  _x86_64_ (32 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          73.96    0.00    3.73    0.03    0.06   22.21

Device:   rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
xvda        0.00     0.23    0.21    0.18     4.52     2.08    34.37     0.00    9.98   13.80    5.42   2.44   0.09
xvdb        0.01     0.00    1.02    8.94   127.97   598.53   145.79     0.00    0.43    1.78    0.28   0.25   0.25
xvdc        0.01     0.00    1.02    8.86   127.79   595.94   146.50     0.00    0.45    1.82    0.30   0.27   0.26
dm-0        0.00     0.00    0.69    2.32    10.47    31.69    28.01     0.01    3.23    0.71    3.98   0.13   0.04
dm-1        0.00     0.00    0.00    0.94     0.01     3.78     8.00     0.33  345.84    0.04  346.81   0.01   0.00
dm-2        0.00     0.00    0.09    0.07     1.35     0.36    22.50     0.00    2.55    0.23    5.62   1.78   0.03
[...]
^C
```

ブロックデバイス(ディスク)に適用されるワークロードと、その結果のパフォーマンスの両方を理解出来る素晴らしいツールです。見方は以下の通り。

r/s, w/s, rkB/s, wkB/s : 秒間にデバイスに送られた読み出し回数、書き込み回数、読み出しキロバイト、書き込みキロバイトを表します。ワークロードの特徴をつかむのに使いましょう。パフォーマンスの問題はたいていの場合、単に過剰な負荷がかけられていることが原因です。
await : I/Oの平均時間のミリ秒表示。これは、アプリケーションが待たされた時間で、キューに入っていた時間と実際のサービス時間の両方を含んでいます。期待した平均時間より長い場合、デバイスが飽和状態か、デバイス自体に問題がある可能性があります。

avgqu-sz : デバイスに対して発行されたリクエストの平均数。(複数のバックエンドディスクの前に立つ仮想デバイスは特に、デバイスはリクエストを通常は並行に処理しますが)1より大きい値は、飽和状態を表します。

%util : デバイスの使用率。これは、実際にはビジーな割合で、デバイスが仕事をした時間を秒ごとに出したものです。デバイスにもよりますが、一般的に60%より大きい値はパフォーマンスの劣化(awaitにも表れます)につながります。100％に近い値は通常、飽和状態を意味します。

ストレージデバイスがたくさんのバックエンドディスクを持つ論理ディスクデバイスの場合、使用率が100%ということは、なんらかのI/Oが100%の時間処理され続けているということである一方で、バックエンドディスクは飽和状態からは程遠い可能性が高く、もっと多くの処理が可能なはずです。

頭にとどめておきたいのは、ディスクI/Oのパフォーマンスが悪いことは、必ずしもアプリケーションの問題になるわけではないということです。I/Oを非同期に実行するために、多くのテクニックがよく使われるので、アプリケーションはブロックされず、レイテンシも直接は影響してこないのです(例、読み出しには先読み、書き込みにはバッファリング)。


## sar -n DEV 1
``` bash
$ sar -n DEV 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015     _x86_64_    (32 CPU)

12:16:48 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
12:16:49 AM      eth0  18763.00   5032.00  20686.42    478.30      0.00      0.00      0.00      0.00
12:16:49 AM        lo     14.00     14.00      1.36      1.36      0.00      0.00      0.00      0.00
12:16:49 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00

12:16:49 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
12:16:50 AM      eth0  19763.00   5101.00  21999.10    482.56      0.00      0.00      0.00      0.00
12:16:50 AM        lo     20.00     20.00      3.25      3.25      0.00      0.00      0.00      0.00
12:16:50 AM   docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
^C
```
このツールで、ワークロードの目安になるネットワークインターフェースのスループットであるrxkB/sとtxkB/sを調べましょう。また、なんらかの制限に達していないかどうかも確認しましょう。上の例では、eth0が22Mbytes/s、つまり176Mbits/secを受信しています(つまり1Gbit/secの制限にひっかかっています)。

このバージョンでは、デバイスの使用率(全二重での上り下り両方での最大値)として%ifutilも表示されていて、これはBrendanのnicstatツールでの計測のためにも使います。ただし、nicstatと同じく、なかなか正しい値を示してはくれず、上の例(0.00)のようにちゃんと動いていないように見える場合が多いようです。

## sar -n TCP,ETCP 1
``` bash
$ sar -n TCP,ETCP 1
Linux 3.13.0-49-generic (titanclusters-xxxxx)  07/14/2015    _x86_64_    (32 CPU)

12:17:19 AM  active/s passive/s    iseg/s    oseg/s
12:17:20 AM      1.00      0.00  10233.00  18846.00

12:17:19 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s
12:17:20 AM      0.00      0.00      0.00      0.00      0.00

12:17:20 AM  active/s passive/s    iseg/s    oseg/s
12:17:21 AM      1.00      0.00   8359.00   6039.00

12:17:20 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s
12:17:21 AM      0.00      0.00      0.00      0.00      0.00
^C
```

これは、いくつかの重要なTCP関連メトリクスの概要です。以下を含んでいます。

active/s : 1秒あたりのローカルから接続を開始したTCPコネクション数(例、connect()による接続)
passive/s : 1秒あたりのリモートから接続を開始したTCPコネクション数(例、accept()による接続)
retrans/s : 1秒あたりのTCP再送数
active/sとpassive/sの数は、それぞれ新しく受け入れたコネクション数と新しく下流に向けて張ったコネクション数で、サーバーの負荷をおおまかに把握するのに便利です。activeを外向き、passiveを内向きと考えるのに便利ですが、厳密に正しいとは言えません(ローカルホストからローカルホストへのコネクションなどを考慮する必要があるなど)。

再送はネットワークあるいはサーバーの問題のサインです。ネットワークの信頼性が低い(例、パブリックなインターネット)か、サーバーが過負荷でパケットをドロップしているかでしょう。上の例では、1秒に1 TCPコネクションしか生成されていません。


## top
``` bash
$ top
top - 00:15:40 up 21:56,  1 user,  load average: 31.09, 29.87, 29.92
Tasks: 871 total,   1 running, 868 sleeping,   0 stopped,   2 zombie
%Cpu(s): 96.8 us,  0.4 sy,  0.0 ni,  2.7 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  25190241+total, 24921688 used, 22698073+free,    60448 buffers
KiB Swap:        0 total,        0 used,        0 free.   554208 cached Mem

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 20248 root      20   0  0.227t 0.012t  18748 S  3090  5.2  29812:58 java
  4213 root      20   0 2722544  64640  44232 S  23.5  0.0 233:35.37 mesos-slave
 66128 titancl+  20   0   24344   2332   1172 R   1.0  0.0   0:00.07 top
  5235 root      20   0 38.227g 547004  49996 S   0.7  0.2   2:02.74 java
  4299 root      20   0 20.015g 2.682g  16836 S   0.3  1.1  33:14.42 java
     1 root      20   0   33620   2920   1496 S   0.0  0.0   0:03.82 init
     2 root      20   0       0      0      0 S   0.0  0.0   0:00.02 kthreadd
     3 root      20   0       0      0      0 S   0.0  0.0   0:05.35 ksoftirqd/0
     5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
     6 root      20   0       0      0      0 S   0.0  0.0   0:06.94 kworker/u256:0
     8 root      20   0       0      0      0 S   0.0  0.0   2:38.05 rcu_sched

```
topコマンドには、これより前に見てきたメトリクスの多くが含まれています。負荷が変わりやすいことを示してくれるここまで見てきたコマンドと違って、ざっくりと確認したいときには便利でしょう。

topの良くないところとしては、時間を追って表れるパターンをつかみにくいことで、これらは連続して出力を出してくれるvmstatやpidstatなどの方がよりはっきりと分かります。間欠的に現れる現象についても、出力を素早く停止(Ctrl-Sで一時停止、Ctrl-Sで再開)できないとスクリーンがクリアされて消えてしまいます。


## ps
### ps -eF f
``` bash
$ ps -eF f
UID        PID  PPID  C    SZ   RSS PSR STIME TTY      STAT   TIME CMD
root         2     0  0     0     0   0 08:46 ?        S      0:00 [kthreadd]
root         3     2  0     0     0   0 08:46 ?        S      0:00  \_ [ksoftirqd/0]
root         6     2  0     0     0   0 08:46 ?        S      0:00  \_ [kworker/u2:0]
root         7     2  0     0     0   0 08:46 ?        S      0:00  \_ [migration/0]
root         8     2  0     0     0   0 08:46 ?        S      0:00  \_ [rcu_bh]
...
root     12009     1  0 56550  5152   0 08:48 ?        Ss     0:00 /usr/sbin/httpd -DFOREGROUND
apache   12010 12009  0 56550  3000   0 08:48 ?        S      0:00  \_ /usr/sbin/httpd -DFOREGROUND
apache   12011 12009  0 56550  3000   0 08:48 ?        S      0:00  \_ /usr/sbin/httpd -DFOREGROUND
apache   12012 12009  0 56550  3000   0 08:48 ?        S      0:00  \_ /usr/sbin/httpd -DFOREGROUND
apache   12013 12009  0 56550  3000   0 08:48 ?        S      0:00  \_ /usr/sbin/httpd -DFOREGROUND
```

### watch -n 1 -d "ps auxw | grep ' R' | grep -v grep"
``` bash
$ watch -n 1 -d "ps auxw | grep ' R' | grep -v grep"
root        10  0.0  0.0      0     0 ?        R    01:56   0:02 [rcu_sched]
root       276  0.0  0.0      0     0 ?        R    01:56   0:01 [xfsaild/sda1]
...

$ watch -n 1 -d "ps auxw | grep ' D' | grep -v grep"
...
```
topコマンドでもある程度のプロセス状態を見ることは可能であるが、より正確にプロセスの状態を見たい場合はpsを利用するとよい。
例えば、実行キューが詰まっている場合はRでgrepしたり、IOが詰まっている場合はDでgrepしたりすると、どのプロセスが悪さをしているかがわかる。

ステータス
* R   Run TASK_RUNNING    実行可能な状態。CPUが空きさえすれば、いつでも実行可能な状態。
* S   Sleep   TASK_INTERRUPTIBLE  割り込み可能な待ち状態。おもに復帰時間が予測不能な長時間の待ち状態。スリープやユーザからの入力待ちなど。
* D   Disk Sleep  TASK_UNINTERRUPTIBLE    割り込み不可能な待ち状態。おもに短時間で復帰する場合の待ち状態。ディスクの入出力待ち。
* T   Stopped TASK_STOPPED    サスペンドシグナルを送られて実行中断になった状態。リジュームされるまでスケジューリングされない。
* Z   Zombie  TASK_ZOMBIE ゾンビ状態。子プロセスが exit して親プロセスにリープされるまでの状態。

### ps -eo [fields]
* 表示フィールドをカスタマイズする
``` bash
$ ps -eo user,sz,rss,minflt,majflt,pcpu,args
```



## /proc/interrupts
```
$ cat /proc/interrupts

$ watch -n 1 -d "cat /proc/interrupts | egrep 'RES|CAL|TLB'"
```
* /proc/interruptsで割り込みを表示できる
* 割り込みを少なくするようにチューニングすることはパフォーマンス向上につながる
* 逆に、割り込みが多すぎるとパケットや、プロセスのつまりの原因になる


## 参考
* [6万ミリ秒でできるLinuxパフォーマンス分析]: https://yakst.com/ja/posts/3601?platform=hootsuite
  [原文]: http://techblog.netflix.com/2015/11/linux-performance-analysis-in-60s.html
* [Linux Performance Tools]: http://techblog.netflix.com/2015/08/netflix-at-velocity-2015-linux.html
